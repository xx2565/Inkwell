<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Xa Inkwell</title><link>https://xx2565.github.io/Inkwell</link><description>daily learning &amp; working</description><copyright>Xa Inkwell</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>https://xx2565.github.io/Inkwell</link></image><lastBuildDate>Fri, 06 Feb 2026 16:58:09 +0000</lastBuildDate><managingEditor>Xa Inkwell</managingEditor><ttl>60</ttl><webMaster>Xa Inkwell</webMaster><item><title>ã€vllmã€‘ çº¿ä¸Šæ¨¡å¼</title><link>https://xx2565.github.io/Inkwell/post/%E3%80%90vllm%E3%80%91%20-xian-shang-mo-shi.html</link><description># APIserveråœ¨æ¡†æ¶ä¸­çš„ä½œç”¨
## è°ƒç”¨é“¾è·¯
vLLMçš„online servingé‡‡ç”¨åˆ†å±‚æ¶æ„ï¼Œä»HTTPæ¥å£åˆ°æ ¸å¿ƒæ¨ç†å¼•æ“çš„å®Œæ•´é“¾è·¯å¦‚ä¸‹ï¼š

å®¢æˆ·ç«¯è¯·æ±‚ â†’ HTTPæœåŠ¡å™¨ â†’ APIè·¯ç”± â†’ æœåŠ¡å±‚ â†’ å¼•æ“å®¢æˆ·ç«¯ â†’ æ ¸å¿ƒå¼•æ“ â†’ æ¨ç†æ‰§è¡Œ

å…·ä½“æ–‡ä»¶é“¾è·¯ï¼š
examples/online_serving/openai_chat_completion_client.py (å®¢æˆ·ç«¯)
    â†“
vllm/entrypoints/openai/api_server.py (HTTPæœåŠ¡å™¨å…¥å£)          å‹æµ‹å…¶æœåŠ¡çš„æ–‡ä»¶åœ°å€
    â†“  
vllm/entrypoints/openai/chat_completion/api_router.py (APIè·¯ç”±)
    â†“
vllm/entrypoints/openai/chat_completion/serving.py (æœåŠ¡å±‚)
    â†“
vllm/v1/engine/async_llm.py (å¼‚æ­¥å¼•æ“å®¢æˆ·ç«¯)
    â†“
vllm/v1/engine/core_client.py (å¼•æ“æ ¸å¿ƒå®¢æˆ·ç«¯)
    â†“
vllm/v1/engine/core.py (æ ¸å¿ƒå¼•æ“)
    â†“
vllm/v1/executor/ (æ¨ç†æ‰§è¡Œå™¨)


å®¢æˆ·ç«¯ (examples/online_serving/openai_chat_completion_client.py)
    â†“ HTTPè¯·æ±‚åˆ° http://localhost:8000/v1/chat/completions
æœåŠ¡å™¨ (vllm/entrypoints/openai/api_server.py)
    â†“ è·¯ç”±åˆ° vllm/entrypoints/openai/chat_completion/api_router.py
    â†“ è°ƒç”¨ vllm/entrypoints/openai/chat_completion/serving.py
    â†“ å§”æ‰˜ç»™ vllm/v1/engine/async_llm.py

## æœåŠ¡æ¨¡å¼å’Œç¨‹åºæ¨¡å¼çš„åŒºåˆ«ï¼š
åœºæ™¯ 1ï¼šå‰ç«¯ç½‘é¡µè¦ç”¨æ¨¡å‹ğŸ‘‰ é‚£åªèƒ½èµ° HTTP  
åœºæ™¯ 2ï¼šå¾ˆå¤šäººåŒæ—¶ç”¨æ¨¡å‹ğŸ‘‰ æ¨¡å‹å¿…é¡»åªåŠ è½½ä¸€æ¬¡  
åœºæ™¯ 3ï¼šæ¨¡å‹è¦ä¸€ç›´å¼€ç€ï¼ˆ7Ã—24ï¼‰ğŸ‘‰ ä¸é€‚åˆåšæœåŠ¡  
API Server å°±æ˜¯ä¸ºäº†è§£å†³ä¸Šé¢è¿™äº›é—®é¢˜  

## ä»€ä¹ˆæ˜¯ FastAPIï¼Ÿ
å¦‚æœæ²¡æœ‰ FastAPIï¼Œä½ è¦æ‰‹å†™å¾ˆå¤šéº»çƒ¦çš„ä¸œè¥¿ï¼š
è§£æ HTTP
è§£æ JSON
æ ¡éªŒå‚æ•°
è¿”å›ç»“æœ
FastAPI å¸®ä½ å…¨åšäº†ã€‚</description><guid isPermaLink="true">https://xx2565.github.io/Inkwell/post/%E3%80%90vllm%E3%80%91%20-xian-shang-mo-shi.html</guid><pubDate>Fri, 06 Feb 2026 08:11:47 +0000</pubDate></item><item><title>gpu coding&amp;arch</title><link>https://xx2565.github.io/Inkwell/post/gpu%20coding%26arch.html</link><description># gemm
```c++
#include &lt;cuda_runtime.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

# define BLOCKSIZE 16

__global__ void gemm_kernel_tile(const float* __restrict__ a,
                                  const float* __restrict__ b,
                                  float* __restrict__ c,
                                  int M, int N, int K){
                                
                                int y = blockIdx.y * blockDim.y + threadIdx.y;
                                int x = blockIdx.x * blockDim.x + threadIdx.x;
                                
                                __shared__ float a_tile[BLOCKSIZE][BLOCKSIZE], b_tile[BLOCKSIZE][BLOCKSIZE];
                                
                                float sum = 0.0f;

                                for(int i=0;i&lt;K/BLOCKSIZE;++i){
                                    // load to sahared memory a
                                    int ax = threadIdx.x + i * BLOCKSIZE;
                                    int ay = y;
                                    if(ax &lt; K &amp;&amp; ay &lt; M){
                                        a_tile[threadIdx.y][threadIdx.x] = a[ay * K + ax];   // â­ï¸ é˜²æ­¢å…±äº«å†…å­˜åæ ‡è¶Šç•Œ
                                    }else{
                                        a_tile[threadIdx.y][threadIdx.x] = 0.0f;
                                    }
                                    // load to sahared memory b
                                    int bx = threadIdx.x;
                                    int by = threadIdx.y + i * BLOCKSIZE;
                                    if(bx &lt; N &amp;&amp; by &lt; K){
                                        b_tile[threadIdx.y][threadIdx.x] = b[by * N + bx];
                                    }else{
                                        b_tile[threadIdx.y][threadIdx.x] = 0.0f;
                                    }

                                    __syncthreads();  // â­ï¸ æ•°æ®åŠ è½½å®Œæˆä¹‹ååŒæ­¥

                                    
                                    for(int i=0;i&lt;BLOCKSIZE;++i){
                                        sum += a_tile[threadIdx.y][i] * b_tile[i][threadIdx.x];
                                    }
                                    __syncthreads();  // â­ï¸ åŒæ­¥ï¼Œé˜²æ­¢æå‰è¿›å…¥ä¸‹ä¸€è½®è®¡ç®—ç„¶åç´¯åŠ é”™è¯¯
                                }
                                if(x &lt; N &amp;&amp; y &lt; M){
                                    c[y * N + x] = sum;
                                }
                            }
                                    


__global__ void gemm_kernel(const float* __restrict__ a,
                            const float* __restrict__ b,
                            float* __restrict__ c,
                            int M, int N, int K) {
    // Each thread computes one element of C
    int row = blockIdx.y * blockDim.y + threadIdx.y; // y -&gt; row in C (0..M-1)
    int col = blockIdx.x * blockDim.x + threadIdx.x; // x -&gt; col in C (0..N-1)

    if (row &lt; M &amp;&amp; col &lt; N) {
        float sum = 0.0f;
        for (int k = 0; k &lt; K; ++k) {
            // A[row][k] * B[k][col]
            sum += a[row * K + k] * b[k * N + col];
        }
        c[row * N + col] = sum;
    }
}

int main() {
    // Matrix dimensions: A(MxK) * B(KxN) = C(MxN)
    const int M = 5000;
    const int K = 6000;
    const int N = 4000;

    const size_t size_a = M * K * sizeof(float);
    const size_t size_b = K * N * sizeof(float);
    const size_t size_c = M * N * sizeof(float);

    // Host memory allocation
    float *h_a = (float*)malloc(size_a);
    float *h_b = (float*)malloc(size_b);
    float *h_c = (float*)malloc(size_c);
    float *h_c_ref = (float*)malloc(size_c); // Optional: CPU reference

    // Initialize host matrices
    for (int i = 0; i &lt; M * K; ++i) h_a[i] = 1.0f; // A all 1s
    for (int i = 0; i &lt; K * N; ++i) h_b[i] = 2.0f; // B all 2s
    for (int i = 0; i &lt; M * N; ++i) h_c[i] = 0.0f; // Initialize to 0

    // Device memory allocation
    float *d_a, *d_b, *d_c;
    cudaError_t err;

    err = cudaMalloc(&amp;d_a, size_a);
    if (err != cudaSuccess) { fprintf(stderr, 'cudaMalloc d_a failed: %s\n', cudaGetErrorString(err)); return 1; }

    err = cudaMalloc(&amp;d_b, size_b);
    if (err != cudaSuccess) { fprintf(stderr, 'cudaMalloc d_b failed: %s\n', cudaGetErrorString(err)); return 1; }

    err = cudaMalloc(&amp;d_c, size_c);
    if (err != cudaSuccess) { fprintf(stderr, 'cudaMalloc d_c failed: %s\n', cudaGetErrorString(err)); return 1; }

    // Copy data from host to device
    cudaMemcpy(d_a, h_a, size_a, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, h_b, size_b, cudaMemcpyHostToDevice);
    cudaMemcpy(d_c, h_c, size_c, cudaMemcpyHostToDevice);

    // Kernel launch configuration
    dim3 blockSize(16, 16); // 256 threads per block
    dim3 gridSize((N + blockSize.x - 1) / blockSize.x,
                  (M + blockSize.y - 1) / blockSize.y);

    // Launch kernel
    gemm_kernel_tile&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(d_a, d_b, d_c, M, N, K);

    // Check for kernel launch errors
    err = cudaGetLastError();
    if (err != cudaSuccess) {
        fprintf(stderr, 'Kernel launch failed: %s\n', cudaGetErrorString(err));
        return 1;
    }

    // Wait for GPU to finish
    cudaDeviceSynchronize();

    // Copy result back to host
    cudaMemcpy(h_c, d_c, size_c, cudaMemcpyDeviceToHost);

    printf('First 10 elements of C (row 0, columns 0ï½9):\n');
    for (int i = 0; i &lt; 10 &amp;&amp; i &lt; N; ++i) {
        printf('C[0][%d] = %.2f\n', i, h_c[i]);
    }
    printf('\n');

    // Optional: Verify result (C should be all 2*K = 400.0f)
    bool correct = true;
    float expected = 2.0f * K; // since A=1, B=2, sum over K terms: 1*2*K
    for (int i = 0; i &lt; M * N; ++i) {
        if (abs(h_c[i] - expected) &gt; 1e-5) {
            correct = false;
            break;
        }
    }

    printf('Matrix multiplication result: %s\n', correct ? 'PASSED' : 'FAILED');
    if (!correct) {
        printf('Example: h_c[0] = %f, expected = %f\n', h_c[0], expected);
    }

    // Cleanup
    free(h_a); free(h_b); free(h_c); free(h_c_ref);
    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);
    // no
    return 0;
}

```

# transpose
```c++
# include &lt;stdio.h&gt;
# include &lt;math.h&gt;

#define BLOCK_SIZE 32
#define M 3000
#define N 1000

__managed__ int matrix[N][M];
__managed__ int gpu_result[M][N];
__managed__ int cpu_result[M][N];

__global__ void gpu_matrix_transpose(int in[N][M], int out[M][N])
{
    int x = threadIdx.x + blockDim.x * blockIdx.x;
    int y = threadIdx.y + blockDim.y * blockIdx.y;

    if( x &lt; M &amp;&amp; y &lt; N)
    {
        out[x][y] = in[y][x];
    }
}

// åˆ›å»ºmè¡Œï¼Œnåˆ—çš„çº¿ç¨‹æ•°é‡ã€ç”±å¤šä¸ªçº¿ç¨‹å—ç»„æˆçš„ã€‘
__global__ void gpu_shared_matrix_transpose(int in[N][M], int out[M][N])
{

    int y = threadIdx.y + blockDim.y * blockIdx.y;
    int x = threadIdx.x + blockDim.x * blockIdx.x;

    __shared__ int ken[BLOCK_SIZE+1][BLOCK_SIZE+1];//ken[32] warp

    // step1ï¼š
    if(x &lt; M &amp;&amp; y &lt; N)
    {   
        // step1ï¼šè¯»åˆ°å…±äº«å†…å­˜
        ken[threadIdx.y][threadIdx.x] = in[y][x];
    }
    __syncthreads();

    // åŸåˆ™ï¼šç›¸é‚»çš„çº¿ç¨‹è®¿é—®ç›¸é‚»çš„åæ ‡

    // step2ï¼š  å—åè½¬ï¼Œå—å†…åæ ‡ä¸å˜
    int x1 = threadIdx.x + blockDim.y * blockIdx.y;
    int y1 = threadIdx.y + blockDim.x * blockIdx.x;
    
    if(x1 &lt; N &amp;&amp; y1 &lt; M)
    {
    // step3ï¼šä»å…±äº«å†…å­˜è¯»åˆ°è¾“å‡ºæ•°æ®
        out[y1][x1] = ken[threadIdx.x][threadIdx.y];//32 bank
    }

}

void cpu_matrix_transpose(int in[N][M], int out[M][N])
{
    for(int y = 0; y &lt; N; y++)
    {
        for(int x = 0; x &lt; M; x++)
        {
            out[x][y] = in[y][x];
        }
    }
}

int main()
{
    for(int y=0; y&lt;N; y++)
    {
        for(int x=0; x&lt;M; x++)
        {
            matrix[y][x] = rand()%1024;
        }
    }

    cudaEvent_t start, stop_gpu, stop_cpu;
    cudaEventCreate(&amp;start);
    cudaEventCreate(&amp;stop_cpu);
    cudaEventCreate(&amp;stop_gpu);

    cudaEventRecord(start);
    cudaEventSynchronize(start);

    dim3 dimGrid((M + BLOCK_SIZE - 1)/BLOCK_SIZE, (N + BLOCK_SIZE -1)/BLOCK_SIZE);
    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);

    for(int i = 0; i &lt; 20; i++)
    {
        // gpu_matrix_transpose&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(matrix, gpu_result);
        gpu_shared_matrix_transpose&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(matrix, gpu_result);
        cudaDeviceSynchronize();
    }

    cudaEventRecord(stop_gpu);
    cudaEventSynchronize(stop_gpu);

    cpu_matrix_transpose(matrix, cpu_result);

    cudaEventRecord(stop_cpu);
    cudaEventSynchronize(stop_cpu);

    float time_cpu, time_gpu;
    cudaEventElapsedTime(&amp;time_gpu, start, stop_gpu);
    cudaEventElapsedTime(&amp;time_cpu, stop_gpu, stop_cpu);

    bool errors = false;
    for(int y = 0; y&lt;M; y++)
    {
        for (int x = 0; x &lt; N; x++)
        {
            if(fabs(cpu_result[y][x] - gpu_result[y][x]) &gt; (1.0e-10))
            {
                errors = true;
            }
        }
        
    }

    printf('Result: %s\n', errors?'Error':'Pass');
    printf('CPU time: %.2f\nGPU time: %.2f\n', time_cpu, time_gpu/20.0);

    return 0;
}
```ã€‚</description><guid isPermaLink="true">https://xx2565.github.io/Inkwell/post/gpu%20coding%26arch.html</guid><pubDate>Sun, 25 Jan 2026 08:57:14 +0000</pubDate></item></channel></rss>